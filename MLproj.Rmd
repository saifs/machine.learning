---
title: "Machine Learning with the HAR Weight Lifting Exercises Dataset"
author: "By S.S."
date: ''
output: html_document
---

## Synopsis

[Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) is emerging as a new field where wearable devices are commonly used to quantify the amount of time an activity is performed. In our analysis, we instead look at how *well* weight lifting exercises were performed in a study. Each individual in the experiment had various accelerometer data collected from devices on different parts of the body while performing barbell exercises in five different ways. We developed machine learning algorithms that predict the way they were performed based on accelerometer data. Our final model that gave us a 100% In Sample accuracy and a 99.0% Out of Sample accuracy was the random forest algorithm with a 10-fold cross-validation repeated 5 times.

## Loading the Data

We first load the required packages *caret* for machine learning and *doParallel* for registering a parallel backend for our training instructions to utilize multi-Core CPUs. We then download the train and test datasets and read them into data frame objects.

```{r message=FALSE, warning=FALSE}
library(caret)
library(doParallel)
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
               destfile="pml-training.csv", mode="wb")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile="pml-testing.csv", mode="wb")
dataTrain <- read.csv("pml-training.csv", na.strings=c("#DIV/0!","NA"))
dataTest <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!","NA"))

```

## Summarizing, Cleaning and Partitioning the Data

First, we see that the train dataset has 160 columns. Our outcome variable that will be predicted, *classe*, consists of values A, B, C, D or E. Each letter (class) refers to one of the five different ways the barbell excercise can be performed. We check for missing values and find that 100 of these columns have missing values in excess of 19216 each.

```{r message=FALSE, warning=FALSE}
dim(dataTrain)
missCols <- colSums(is.na(dataTrain))
names(missCols[missCols>=19216])
```

Next, we see that the first 7 columns contain data that is not relevant to the accelerometers.

```{r message=FALSE, warning=FALSE}
colnames(dataTrain[1:7])
```

We clean both the train and test datasets by removing the first 7 columns and since we do not want predictors almost completely filled with missing values, we also remove 100 columns. The final datasets have 53 columns.

```{r message=FALSE, warning=FALSE}
dataTrain <- dataTrain[,complete.cases(t(dataTrain))]
dataTrain <- dataTrain[, -c(1:7)]
dataTest <- dataTest[,complete.cases(t(dataTest))]
dataTest <- dataTest[, -c(1:7)]
dim(dataTrain)
dim(dataTest)
```

Finally, we partition the training set into training and testing subsets to be used for training our models and estimating our errors. The data is split with 60% training and 40% testing.

```{r message=FALSE, warning=FALSE}
set.seed(24340)
inTrain = createDataPartition(y=dataTrain$classe, p = 0.6,list=FALSE)
training = dataTrain[inTrain,]
testing = dataTrain[-inTrain,]
```

## Fitting Predictive Models

We implement various machine learning algorithms using the *caret* package:  Linear discriminant analysis, naive Bayes, CART, random forest, partial least squares and stochastic gradient boosting. In this first step, we use default tuning parameters such as bootstrapping resamples. The models are stored in a list. We also calculate the confusion matrix to find the In Sample Error which will be used to determine a final model. Parallel processing is used to speed up computation time.

```{r message=FALSE, warning=FALSE, results='hide'}
cl <- makeCluster(detectCores()) 
registerDoParallel(cl)      # Register parallel backend
model <- list()
modelError <- list()
set.seed(24340)
model[[1]] <- train(classe ~ ., data=training, method="lda")
modelError[[1]] <- confusionMatrix(predict(model[[1]], newdata=training), training$classe)
set.seed(24340)
model[[2]] <- train(classe ~ ., data=training, method="nb")
modelError[[2]] <- confusionMatrix(predict(model[[2]], newdata=training), training$classe)
set.seed(24340)
model[[3]] <- train(classe ~ ., data=training, method="rpart")
modelError[[3]] <- confusionMatrix(predict(model[[3]], newdata=training), training$classe)
set.seed(24340)
model[[4]] <- train(classe ~ ., data=training, method="rf")
modelError[[4]] <- confusionMatrix(predict(model[[4]], newdata=training), training$classe)
set.seed(24340)
model[[5]] <- train(classe ~ ., data=training, method="pls")
modelError[[5]] <- confusionMatrix(predict(model[[5]], newdata=training), training$classe)
set.seed(24340)
model[[6]] <- train(classe ~ ., data=training, method="gbm")
modelError[[6]] <- confusionMatrix(predict(model[[6]], newdata=training), training$classe)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ctrl <- trainControl(method="repeatedcv", number=10, repeats=5)
set.seed(24340)
model[[7]] <- train(classe ~ ., data=training, method="rf", trControl=ctrl)
modelError[[7]] <- confusionMatrix(predict(model[[7]], newdata=testing), testing$classe)
stopCluster(cl)     # Stop parallel backend
```

We generate a table that summarizes our predictive models.

```{r message=FALSE, warning=FALSE, results='asis'}
algorithm <- character(length(model))
resample <- character(length(model))
error <- numeric(length(model))
for (i in 1:length(model)) {
    algorithm[i] <- as.character(model[[i]]$modelInfo[1])
    resample[i] <- ifelse(i==7,paste(as.character(model[[i]]$control[1]), " (",
                                      as.character(model[[i]]$control[2]), " folds, ",
                                      as.character(model[[i]]$control[3]), " repeats)",
                                      sep=""),
                          paste(as.character(model[[i]]$control[1]), " (",
                                      as.character(model[[i]]$control[2]), " iterations)",
                                      sep=""))
    error[i] <- paste(format(round(100-modelError[[i]]$overall[1]*100,1), nsmall=1),"%",sep="")
}
modelInfo <- data.frame(algorithm, resample, error)
colnames(modelInfo) <- c("Algorithm", "Resampling", "In Sample Error (1-Accuracy)")
knitr::kable(modelInfo[1:6,], caption="Fitted Predictive Models")
```

This table automatically populates specific attributes fetched from each model. We can see that the Random Forest model has a 0.0% In Sample Error (100% accuracy). This model is sufficient for us because of its perfect prediction of the outcome variable *classe* from the training subset. We will tune this model to build our final model. 

## Final Model Results

Our final model will use tuning that includes 10-fold cross-validation repeated 5 times to obtain our Out of Sample Error on the testing subset.

```{r eval=FALSE, message=FALSE, warning=FALSE}
ctrl <- trainControl(method="repeatedcv", number=10, repeats=5)
set.seed(24340)
model[[7]] <- train(classe ~ ., data=training, method="rf", trControl=ctrl)
modelError[[7]] <- confusionMatrix(predict(model[[7]], newdata=testing), testing$classe)
stopCluster(cl)     # Stop parallel backend
```

We show the results of our final model.

```{r message=FALSE, warning=FALSE}
colnames(modelInfo) <- c("Algorithm", "Resampling", "Out of Sample Error (1-Accuracy)")
knitr::kable(modelInfo[7,], caption="Final Predictive Model", row.names=FALSE)
```

This table shows that our final model (with cross-validation) has a 1.0% Out of Sample Error (99.0% accuracy) with the testing subset in predicting the *classe* outcome. The result is very positive.

```{r message=FALSE, warning=FALSE}
knitr::kable(modelError[[7]]$table, caption="Confusion Matrix")
```

The confusion matrix above shows which predictions on the testing subset were correct and which were not. Our predictions are on the columns and the rows are the actual values. The non-diagonal elements are the errors.

```{r message=FALSE, warning=FALSE}
model[[7]]
```

The summary of our final model shows the varying accuracy metric depends on the predictors. 

```{r message=FALSE, warning=FALSE}
plot(model[[7]])
```

The plot shows the relationship between the number of randomly selected predictors and the accuracy. As we can see, the accuracy is highest when *mtry*, the number of variables available for splitting at each tree node is 27 (stated in the summary above also).

We then use our final model to predict the outcome *classe* on our separate test dataset. The results are submitted to the project page where all 20 tests are correct.

```{r message=FALSE, warning=FALSE}
answers <- as.character(predict(model[[7]], newdata=dataTest))
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(answers)
```

## Conclusions

The HAR weight lifting exercises dataset has accelerometer readings that allow us to predict one of the five different ways of lifting barbells. Our data analysis yielded the random forest algorithm with 10-fold cross-validation repeated 5 times as our best model. The results were very positive with a 0% In Sample Error and 1.0% Out of Sample Error. Our final model was also able to correctly predict all outcomes of the separate test dataset required for submission as part of the project.